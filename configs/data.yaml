# Data configuration for RefCOCO phrase grounding

# RefCOCO dataset from HuggingFace
dataset:
  # HuggingFace dataset name
  name: "lmms-lab/RefCOCO"

  # RefCOCO variant: "refcoco", "refcoco+", or "refcocog"
  # - refcoco: Original dataset
  # - refcoco+: No location words in expressions
  # refcocog: Longer, more complex expressions
  variant: "refcoco"

# Budget-aware sampling
# Set to null to use all available data, or specify a number to limit
sampling:
  # Training samples
  max_train_samples: null  # null = all, or set to 5000, 10000, etc.

  # Validation samples
  max_val_samples: 2000

  # Test samples
  max_test_samples: 2000

# Data processing
processing:
  # Image resize (shorter edge), set to null to disable
  # Note: Qwen3-VL processor will handle final resizing
  resize: null

  # Data augmentation for training
  # (Keep minimal for phrase grounding - bbox correspondence is critical)
  augment_train: false

# DataLoader settings (for local MPS)
dataloader:
  batch_size: 4
  num_workers: 0  # Use 0 for MPS compatibility
  pin_memory: false  # MPS doesn't support pin_memory
  shuffle: true

# Runpod overrides (for cloud GPU training)
# These settings are used when --prefer_cuda flag is set
runpod:
  # Smaller batch size with gradient accumulation
  # Effective batch size = batch_size * gradient_accumulation_steps
  batch_size: 2
  gradient_accumulation_steps: 4  # Effective: 8

  # Can use more workers on GPU instances
  num_workers: 2

# Budget presets (copy these values to sampling.max_train_samples)
budget_presets:
  # Local debugging (free, ~10-20 min)
  local_debug:
    max_train_samples: 500
    max_val_samples: 100
    num_epochs: 1

  # Runpod conservative (~$3-5, 3-6 hours)
  runpod_conservative:
    max_train_samples: 5000
    max_val_samples: 1000
    num_epochs: 2

  # Runpod moderate (~$6-9, 8-12 hours)
  runpod_moderate:
    max_train_samples: 15000
    max_val_samples: 2000
    num_epochs: 2
    max_steps: 5000  # Early stopping
