"""
Inference utilities for predicting bounding boxes on single images.
"""
from PIL import Image
from pathlib import Path
from typing import Dict, Optional, Tuple, Any
import torch

from ..data.box_utils import denormalize_bbox, json_to_bbox, extract_json_from_text
from ..common.viz import draw_bbox_on_image
from .prompts import build_main_subject_prompt


def predict_main_subject_bbox(
    image: Image.Image,
    model: Any,
    processor: Any,
    device: str = "mps",
    max_new_tokens: int = 100,
    visualize: bool = False
) -> Dict:
    """
    Run inference on a single image to predict the main subject bounding box.

    Args:
        image: PIL Image
        model: Trained model (PEFT or merged)
        processor: Qwen3-VL processor
        device: Device to run on
        max_new_tokens: Maximum tokens to generate
        visualize: Whether to create a visualization with the predicted box

    Returns:
        Dictionary containing:
        - bbox_norm: Normalized bbox (x_min, y_min, x_max, y_max) or None
        - bbox_px: Pixel bbox (x_min, y_min, x_max, y_max) or None
        - raw_response: Raw text generated by the model
        - success: Whether bbox was successfully parsed
        - visualization: PIL Image with bbox drawn (if visualize=True)
    """
    # Ensure model is in eval mode
    model.eval()

    # Get image dimensions
    width, height = image.size

    # Build prompt
    prompt = build_main_subject_prompt()

    # Format messages
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }
    ]

    # Apply chat template
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Process inputs
    inputs = processor(
        text=[text],
        images=[image],
        return_tensors="pt",
        padding=True
    )

    # Move to device
    inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v
              for k, v in inputs.items()}

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,  # Greedy decoding
            pad_token_id=processor.tokenizer.pad_token_id,
            eos_token_id=processor.tokenizer.eos_token_id,
        )

    # Decode output (only the generated part)
    generated_ids = outputs[0][inputs["input_ids"].shape[1]:]
    raw_response = processor.tokenizer.decode(
        generated_ids,
        skip_special_tokens=True
    )

    # Try to extract JSON from response
    json_str = extract_json_from_text(raw_response)

    # Parse JSON to bbox
    bbox_norm = None
    bbox_px = None
    success = False

    if json_str:
        bbox_norm = json_to_bbox(json_str)
    else:
        # Try parsing raw response directly
        bbox_norm = json_to_bbox(raw_response)

    if bbox_norm is not None:
        success = True
        # Convert to pixel coordinates
        bbox_px = denormalize_bbox(bbox_norm, width, height)

    # Create visualization if requested
    visualization = None
    if visualize and bbox_norm is not None:
        visualization = draw_bbox_on_image(
            image,
            bbox_norm,
            color="red",
            width=3,
            label="Predicted Main Subject"
        )

    return {
        "bbox_norm": bbox_norm,
        "bbox_px": bbox_px,
        "raw_response": raw_response,
        "success": success,
        "visualization": visualization,
        "image_width": width,
        "image_height": height,
    }


def batch_predict(
    image_paths: list[Path],
    model: Any,
    processor: Any,
    device: str = "mps",
    save_visualizations: bool = False,
    output_dir: Optional[Path] = None
) -> list[Dict]:
    """
    Run inference on multiple images.

    Args:
        image_paths: List of paths to images
        model: Trained model
        processor: Qwen3-VL processor
        device: Device to run on
        save_visualizations: Whether to save visualization images
        output_dir: Directory to save visualizations (required if save_visualizations=True)

    Returns:
        List of prediction dictionaries
    """
    results = []

    for img_path in image_paths:
        # Load image
        image = Image.open(img_path).convert("RGB")

        # Predict
        result = predict_main_subject_bbox(
            image,
            model,
            processor,
            device=device,
            visualize=save_visualizations
        )

        # Add image path to result
        result["image_path"] = str(img_path)

        # Save visualization if requested
        if save_visualizations and result["visualization"] is not None:
            if output_dir is None:
                raise ValueError("output_dir must be provided when save_visualizations=True")

            output_dir.mkdir(parents=True, exist_ok=True)
            vis_path = output_dir / f"{img_path.stem}_prediction.png"
            result["visualization"].save(vis_path)
            result["visualization_path"] = str(vis_path)

        results.append(result)

    return results


def load_model_and_predict(
    image_path: Path,
    checkpoint_path: Optional[Path] = None,
    model_name: str = "Qwen/Qwen3-VL-2B-Instruct",
    device: str = "mps",
    visualize: bool = True
) -> Dict:
    """
    Convenience function to load model and run prediction in one call.

    Args:
        image_path: Path to input image
        checkpoint_path: Path to LoRA checkpoint (if None, uses base model)
        model_name: Base model name
        device: Device to use
        visualize: Whether to create visualization

    Returns:
        Prediction dictionary
    """
    from .model_qwen3 import load_qwen3_vl_with_lora, load_lora_weights

    print(f"Loading model: {model_name}")

    # Load base model
    model, processor = load_qwen3_vl_with_lora(
        model_name=model_name,
        device=device
    )

    # Load checkpoint if provided
    if checkpoint_path is not None:
        print(f"Loading checkpoint: {checkpoint_path}")
        model = load_lora_weights(model, str(checkpoint_path))

    # Load image
    print(f"Loading image: {image_path}")
    image = Image.open(image_path).convert("RGB")

    # Predict
    print("Running inference...")
    result = predict_main_subject_bbox(
        image,
        model,
        processor,
        device=device,
        visualize=visualize
    )

    # Print results
    print("\n" + "=" * 50)
    print("PREDICTION RESULTS")
    print("=" * 50)
    print(f"Image: {image_path}")
    print(f"Size: {result['image_width']} x {result['image_height']}")
    print(f"Success: {result['success']}")
    if result['success']:
        print(f"Normalized bbox: {result['bbox_norm']}")
        print(f"Pixel bbox: {result['bbox_px']}")
    print(f"Raw response: {result['raw_response']}")
    print("=" * 50)

    return result
