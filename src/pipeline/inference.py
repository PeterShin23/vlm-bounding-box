"""
Inference utilities for RefCOCO phrase grounding - predicting bounding boxes from referring expressions.
"""
import csv
import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch
from PIL import Image

from ..data.box_utils import (
    denormalize_bbox,
    json_to_bbox,
    extract_json_from_text,
)
from ..common.viz import draw_bbox_on_image
from .prompts import build_grounding_prompt


def _sanitize_normalized_bbox(
    bbox: Optional[Tuple[float, float, float, float]],
    clip: bool = True,
    min_span: float = 1e-4,
) -> Optional[Tuple[float, float, float, float]]:
    """
    Ensure bbox coordinates are ordered, optionally clipped to [0, 1],
    and represent a non-degenerate rectangle.
    """
    if bbox is None:
        return None

    x_min, y_min, x_max, y_max = bbox

    # Ensure min <= max by swapping when necessary
    x_min, x_max = min(x_min, x_max), max(x_min, x_max)
    y_min, y_max = min(y_min, y_max), max(y_min, y_max)

    if clip:
        x_min = min(max(x_min, 0.0), 1.0)
        y_min = min(max(y_min, 0.0), 1.0)
        x_max = min(max(x_max, 0.0), 1.0)
        y_max = min(max(y_max, 0.0), 1.0)

    # Reject degenerate boxes
    if (x_max - x_min) < min_span or (y_max - y_min) < min_span:
        return None

    return (x_min, y_min, x_max, y_max)


def log_attempts_to_csv(log_path: Path, prediction: Dict[str, Any]) -> None:
    """Append attempt-level details to a CSV log."""
    attempt_logs = prediction.get("attempt_logs", [])
    if not attempt_logs:
        return

    log_path.parent.mkdir(parents=True, exist_ok=True)
    fieldnames = [
        "image_path",
        "phrase",
        "attempt",
        "used_sampling",
        "success",
        "sanitized_bbox",
        "parsed_bbox",
        "raw_response",
    ]
    file_exists = log_path.exists()
    with log_path.open("a", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()

        for log in attempt_logs:
            row_bbox = log.get("sanitized_bbox")
            writer.writerow(
                {
                    "image_path": prediction.get("image_path"),
                    "phrase": prediction.get("phrase"),
                    "attempt": log.get("attempt"),
                    "used_sampling": log.get("used_sampling"),
                    "success": row_bbox is not None,
                    "sanitized_bbox": json.dumps(row_bbox),
                    "parsed_bbox": json.dumps(log.get("parsed_bbox")),
                    "raw_response": log.get("raw_response"),
                }
            )


def predict_grounding_bbox(
    image: Image.Image,
    phrase: str,
    model: Any,
    processor: Any,
    device: str = "mps",
    max_new_tokens: int = 100,
    visualize: bool = False,
    max_attempts: int = 2,
    sampling_temperature: float = 0.7,
    clip_predictions: bool = True,
) -> Dict:
    """
    Run inference on a single image to predict bounding box for a referring expression.

    Args:
        image: PIL Image
        phrase: Referring expression (e.g., "the red car on the left")
        model: Trained model (PEFT or merged)
        processor: Qwen3-VL processor
        device: Device to run on
        max_new_tokens: Maximum tokens to generate
        visualize: Whether to create a visualization with the predicted box
        max_attempts: Retry generations when outputs are invalid (subsequent attempts sample)
        sampling_temperature: Temperature used for retries (ignored for first greedy pass)
        clip_predictions: Clip decoded boxes to [0, 1] and drop degenerate boxes

    Returns:
        Dictionary containing:
        - bbox_norm: Normalized bbox (x_min, y_min, x_max, y_max) or None
        - bbox_px: Pixel bbox (x_min, y_min, x_max, y_max) or None
        - phrase: The referring expression used
        - raw_response: Raw text generated by the model
        - success: Whether bbox was successfully parsed
        - visualization: PIL Image with bbox drawn (if visualize=True)
        - attempt_logs: Debug info for each decode attempt
    """
    # Ensure model is in eval mode
    model.eval()

    # Get image dimensions
    width, height = image.size

    # Build phrase-conditional prompt
    prompt = build_grounding_prompt(phrase)

    # Format messages
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }
    ]

    attempt_logs: List[Dict[str, Any]] = []
    bbox_norm = None
    raw_response = ""

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )

    base_inputs = processor(
        text=[text],
        images=[image],
        return_tensors="pt",
        padding=True,
    )

    base_inputs = {
        k: v.to(device) if isinstance(v, torch.Tensor) else v
        for k, v in base_inputs.items()
    }

    for attempt_idx in range(max(1, max_attempts)):
        inputs = base_inputs

        do_sample = attempt_idx > 0
        gen_kwargs = {
            "max_new_tokens": max_new_tokens,
            "do_sample": do_sample,
            "pad_token_id": processor.tokenizer.pad_token_id,
            "eos_token_id": processor.tokenizer.eos_token_id,
        }
        if do_sample:
            gen_kwargs.update(
                temperature=sampling_temperature,
                top_p=0.9,
            )

        with torch.no_grad():
            outputs = model.generate(**inputs, **gen_kwargs)

        generated_ids = outputs[0][inputs["input_ids"].shape[1]:]
        raw_response = processor.tokenizer.decode(
            generated_ids,
            skip_special_tokens=True
        )

        json_str = extract_json_from_text(raw_response)
        parsed_bbox = json_to_bbox(json_str) if json_str else json_to_bbox(raw_response)
        bbox_norm = _sanitize_normalized_bbox(parsed_bbox, clip=clip_predictions)

        attempt_logs.append(
            {
                "attempt": attempt_idx + 1,
                "used_sampling": do_sample,
                "raw_response": raw_response,
                "parsed_bbox": parsed_bbox,
                "sanitized_bbox": bbox_norm,
            }
        )

        if bbox_norm is not None:
            break

    bbox_px = None
    success = bbox_norm is not None
    if success:
        bbox_px = denormalize_bbox(bbox_norm, width, height)

    visualization = None
    if visualize and bbox_norm is not None:
        visualization = draw_bbox_on_image(
            image,
            bbox_norm,
            color="red",
            width=3,
            label=f"Predicted: {phrase[:30]}..." if len(phrase) > 30 else phrase,
        )

    return {
        "bbox_norm": bbox_norm,
        "bbox_px": bbox_px,
        "phrase": phrase,
        "raw_response": raw_response,
        "success": success,
        "visualization": visualization,
        "image_width": width,
        "image_height": height,
        "attempt_logs": attempt_logs,
    }


def batch_predict(
    image_phrase_pairs: list[Tuple[Path, str]],
    model: Any,
    processor: Any,
    device: str = "mps",
    save_visualizations: bool = False,
    output_dir: Optional[Path] = None,
    attempt_log_path: Optional[Path] = None,
) -> list[Dict]:
    """
    Run inference on multiple image-phrase pairs.

    Args:
        image_phrase_pairs: List of (image_path, phrase) tuples
        model: Trained model
        processor: Qwen3-VL processor
        device: Device to run on
        save_visualizations: Whether to save visualization images
        output_dir: Directory to save visualizations (required if save_visualizations=True)
        attempt_log_path: Optional CSV path to append attempt-level diagnostics

    Returns:
        List of prediction dictionaries
    """
    results = []

    for img_path, phrase in image_phrase_pairs:
        # Load image
        image = Image.open(img_path).convert("RGB")

        # Predict
        result = predict_grounding_bbox(
            image,
            phrase,
            model,
            processor,
            device=device,
            visualize=save_visualizations
        )

        # Add image path to result
        result["image_path"] = str(img_path)

        if attempt_log_path is not None:
            log_attempts_to_csv(
                attempt_log_path,
                result,
            )

        # Save visualization if requested
        if save_visualizations and result["visualization"] is not None:
            if output_dir is None:
                raise ValueError("output_dir must be provided when save_visualizations=True")

            output_dir.mkdir(parents=True, exist_ok=True)
            vis_path = output_dir / f"{img_path.stem}_prediction.png"
            result["visualization"].save(vis_path)
            result["visualization_path"] = str(vis_path)

        results.append(result)

    return results


def load_model_and_predict(
    image_path: Path,
    phrase: str,
    checkpoint_path: Optional[Path] = None,
    model_name: str = "Qwen/Qwen3-VL-2B-Instruct",
    device: str = "mps",
    visualize: bool = True
) -> Dict:
    """
    Convenience function to load model and run phrase grounding prediction in one call.

    Args:
        image_path: Path to input image
        phrase: Referring expression (e.g., "the red car on the left")
        checkpoint_path: Path to LoRA checkpoint (if None, uses base model)
        model_name: Base model name
        device: Device to use
        visualize: Whether to create visualization

    Returns:
        Prediction dictionary
    """
    from .model_qwen3 import load_qwen3_vl_with_lora, load_lora_weights

    print(f"Loading model: {model_name}")

    # Load base model
    model, processor = load_qwen3_vl_with_lora(
        model_name=model_name,
        device=device
    )

    # Load checkpoint if provided
    if checkpoint_path is not None:
        print(f"Loading checkpoint: {checkpoint_path}")
        model = load_lora_weights(model, str(checkpoint_path))

    # Load image
    print(f"Loading image: {image_path}")
    image = Image.open(image_path).convert("RGB")

    # Predict
    print(f"Running inference with phrase: \"{phrase}\"")
    result = predict_grounding_bbox(
        image,
        phrase,
        model,
        processor,
        device=device,
        visualize=visualize
    )

    # Print results
    print("\n" + "=" * 50)
    print("PREDICTION RESULTS")
    print("=" * 50)
    print(f"Image: {image_path}")
    print(f"Phrase: \"{phrase}\"")
    print(f"Size: {result['image_width']} x {result['image_height']}")
    print(f"Success: {result['success']}")
    if result['success']:
        print(f"Normalized bbox: {result['bbox_norm']}")
        print(f"Pixel bbox: {result['bbox_px']}")
    print(f"Raw response: {result['raw_response']}")
    print("=" * 50)

    return result
